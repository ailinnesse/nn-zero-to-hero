{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNQUpTSD3MF+zNUVc04Dya",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailinnesse/nn-zero-to-hero/blob/main/makemore_backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AZKfdZvycqvA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the file\n",
        "words = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "fbLbgIQmexaT",
        "outputId": "6f187b0b-a5a4-4ab0-f214-f5f8180883a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-296d9435-fec4-4449-950f-709972b83e2b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-296d9435-fec4-4449-950f-709972b83e2b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving names.txt to names.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGnrmSF7ezfO",
        "outputId": "022e0d33-cfef-4669-c47d-aefa6c408421"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQe0PBMeeziF",
        "outputId": "fd5f590c-0c4b-4498-b22a-8fa15bc34efc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsBgdLz-ezlA",
        "outputId": "f4830b37-41d9-454c-d451-bd5d90fa647c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "7LvxCRAje98j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMYZ_YDVfC6b",
        "outputId": "b1c3febb-dbf8-4155-afbc-42fcc344dc3c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ],
      "metadata": {
        "id": "yvgn-pW4fJS5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teTKdiL_fLkM",
        "outputId": "1feae9dd-2c6d-4d8c-e4b3-f7010db18db0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3379, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logprobs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43JmR94UgpW2",
        "outputId": "c6517822-5b3e-483b-aa9f-83cdd3d8b8ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0/n\n",
        "\n",
        "dprobs = (1.0 / probs) * dlogprobs\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "dnorm_logits = counts * dcounts\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1.0 - h**2) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "dbndiff += (2*bndiff) * dbndiff2\n",
        "dhprebn = dbndiff.clone()\n",
        "dbnmeani = (-dbndiff).sum(0)\n",
        "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb = dembcat.view(emb.shape)\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k,j]\n",
        "    dC[ix] += demb[k,j]"
      ],
      "metadata": {
        "id": "4PKtHchqfP5l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imEoITj5hrJp",
        "outputId": "070aac77-d17b-4a47-ca8e-8c94580c6ab1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icqRk-CHiCLz",
        "outputId": "66240ae1-325f-4073-b017-bc1dbe3a2925"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.3378548622131348 diff: 2.384185791015625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "dlogits = F.softmax(logits, 1)\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3l0oTnXTBkc",
        "outputId": "eed83c87-654c-4ca1-a26d-097b68fb18ce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 6.28642737865448e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape, Yb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D-Eqp2u8kf3",
        "outputId": "6636281d-611d-4d71-cc37-82f8b815e4bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(logits, 1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRRAW0Wr8pgK",
        "outputId": "7bf2396e-dfa7-41b1-fb2b-2b25bdcd2140"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0747, 0.0821, 0.0207, 0.0475, 0.0203, 0.0818, 0.0224, 0.0348, 0.0169,\n",
              "        0.0336, 0.0382, 0.0397, 0.0384, 0.0267, 0.0355, 0.0147, 0.0093, 0.0196,\n",
              "        0.0162, 0.0550, 0.0474, 0.0228, 0.0253, 0.0693, 0.0602, 0.0255, 0.0211],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0] * n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b_JC5NI8rRr",
        "outputId": "15a0f274-e592-4734-bc4d-21d8a95c0ef2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0747,  0.0821,  0.0207,  0.0475,  0.0203,  0.0818,  0.0224,  0.0348,\n",
              "        -0.9831,  0.0336,  0.0382,  0.0397,  0.0384,  0.0267,  0.0355,  0.0147,\n",
              "         0.0093,  0.0196,  0.0162,  0.0550,  0.0474,  0.0228,  0.0253,  0.0693,\n",
              "         0.0602,  0.0255,  0.0211], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6SkJNcT8rVm",
        "outputId": "a198ab9a-dff0-49e7-984d-dd2c8d025ec7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.5879e-09, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "lGWN6gEM8rZg",
        "outputId": "e1addc87-b894-40f4-a091-d68473fd8d1c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7c85c548c220>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZZJN_239RtJ",
        "outputId": "5f79382c-7414-44ef-b5f0-50e393ad7047"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGKlpOSY9R2K",
        "outputId": "33f08e39-0049-4311-8a95-697f503e8db5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idm92TdL9R7T",
        "outputId": "118e48d2-a2d6-48e3-d48b-6c3916872ec2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]),\n",
              " torch.Size([1, 64]),\n",
              " torch.Size([1, 64]),\n",
              " torch.Size([32, 64]),\n",
              " torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "  # kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    dlogits = F.softmax(logits, 1)\n",
        "    dlogits[range(n), Yb] -= 1\n",
        "    dlogits /= n\n",
        "    # 2nd layer backprop\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    # tanh\n",
        "    dhpreact = (1.0 - h**2) * dh\n",
        "    # batchnorm backprop\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "    # 1st layer\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "    # embedding\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for k in range(Xb.shape[0]):\n",
        "      for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k,j]\n",
        "        dC[ix] += demb[k,j]\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "\n",
        "   # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "   #    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpxKriFP-bS2",
        "outputId": "a07b5e21-f519-4c1f-8ad6-9711713d959d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7933\n",
            "  10000/ 200000: 2.1758\n",
            "  20000/ 200000: 2.3633\n",
            "  30000/ 200000: 2.4327\n",
            "  40000/ 200000: 2.0000\n",
            "  50000/ 200000: 2.4147\n",
            "  60000/ 200000: 2.4044\n",
            "  70000/ 200000: 2.0633\n",
            "  80000/ 200000: 2.3755\n",
            "  90000/ 200000: 2.1539\n",
            " 100000/ 200000: 1.9727\n",
            " 110000/ 200000: 2.2928\n",
            " 120000/ 200000: 2.0045\n",
            " 130000/ 200000: 2.4822\n",
            " 140000/ 200000: 2.3110\n",
            " 150000/ 200000: 2.1624\n",
            " 160000/ 200000: 1.9928\n",
            " 170000/ 200000: 1.8303\n",
            " 180000/ 200000: 2.0395\n",
            " 190000/ 200000: 1.9009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# useful for checking your gradients\n",
        "#for p,g in zip(parameters, grads):\n",
        "#  cmp(str(tuple(p.shape)), g, p)"
      ],
      "metadata": {
        "id": "ifm2Lqu3-beg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
      ],
      "metadata": {
        "id": "TokN9CGjBHbg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJn6DICbBIwE",
        "outputId": "972c2d43-66e5-438c-ae0e-d6b1c3a352ef"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.0716304779052734\n",
            "val 2.107985496520996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # ------------\n",
        "      # forward pass:\n",
        "      # Embedding\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # ------------\n",
        "      # Sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQwNb2EdBMcj",
        "outputId": "a9138d24-13d5-4077-87ae-4899b1127e86"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mona.\n",
            "mayah.\n",
            "see.\n",
            "madhayla.\n",
            "reisha.\n",
            "endraegan.\n",
            "chedielin.\n",
            "shi.\n",
            "jen.\n",
            "eden.\n",
            "estanaraelyn.\n",
            "malaia.\n",
            "noshubergshira.\n",
            "sten.\n",
            "joselle.\n",
            "joberlyn.\n",
            "brence.\n",
            "ryyah.\n",
            "fael.\n",
            "yuma.\n"
          ]
        }
      ]
    }
  ]
}